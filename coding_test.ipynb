{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Test - Listing title classifier\n",
    "This is my solution for the first part of the challenge _\"Junior Applied Machine Learning Technical Test\"_.\n",
    "\n",
    "#### Challenge statement reminder\n",
    "The challenge definition has been made in the following way:\n",
    "* design a text classifier which will be able to classify a listing title in two categories: ASSET or NON-ASSET ((is the product or not, respectively).\n",
    "* Three datasets for three different products are provided. The goal is to create a single model for each product using the same method: same algorithm, different data.\n",
    "* Try at least two or three different methods and compare the results, explaining the advantages and disadvantages for each method. Additionally, provide at least one graphic with the algorithm comparison.\n",
    "* (Optional) Take into account training and prediction time. How long does the algorithm take to train and predict? Does your selected algorithm work properly under heavy time constraints?\n",
    "\n",
    "#### Prerequisites\n",
    "As requested in the challenge statment, the code as to be provided in a Jupyter/Google Colab notebook, available in a GitHub repository.\n",
    "See the repository's README file for information on how to run this code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "First, let's load all the necessary modules, specify some configurations and check if the data folder exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all necessary modules.\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt  # Matlab-style plotting\n",
    "import seaborn as sns # data visualization\n",
    "sns.set(style=\"ticks\")\n",
    "\n",
    "import re # special chars removal\n",
    "from nltk.corpus import stopwords # stopwords removal\n",
    "\n",
    "# Check stopwords installation\n",
    "if not stopwords:\n",
    "    import nltk\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) # Limiting floats output to 3 decimal points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data folder\n",
    "DATA_FOLDER = './data/'\n",
    "\n",
    "# Avaiable products\n",
    "PRODUCTS = {\n",
    "    'MAVIC PRO': 'mavic_pro_data.dat',\n",
    "    'ONEPLUS': 'oneplus_data.dat',\n",
    "    'ROOMBA 900 BRUSH': 'roomba_900_brush.dat',\n",
    "}\n",
    "\n",
    "# Check data folder existance and dataset files\n",
    "import os\n",
    "assert os.path.isdir(DATA_FOLDER), 'Data folder does not exist!'\n",
    "assert PRODUCTS['MAVIC PRO'] in os.listdir(DATA_FOLDER), 'Mavic Pro data not in data folder!'\n",
    "assert PRODUCTS['ONEPLUS'] in os.listdir(DATA_FOLDER), 'OnePlus data not in data folder!'\n",
    "assert PRODUCTS['ROOMBA 900 BRUSH'] in os.listdir(DATA_FOLDER), 'Roomba 900 Brush data not in data folder!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The classifier output categories\n",
    "classification_categories = ['ASSET', 'NON-ASSET']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data\n",
    "It's time to load the datasets. The dataset formats description cames directly from the challenge statements.\n",
    "Dataset files are in TSV format, having three columns per row:\n",
    "* *url*: the url from which the info was extracted.\n",
    "* *Title*: the title for the listing.\n",
    "* *Label*: is 0 for a positive sample (or sample belonging to the protected product) and 1 for negative samples (or sample not belonging to the protected product).\n",
    "\n",
    "Usually, to load a dataset from file into a Pandas DataFrame, the functions *read_csv* or (in this case) *read_table* are used.\n",
    "However, with this particular datasets some error occured during the parsing, probably because there some tabular delimiter inside the middle column (and the tsv format use tabs to differentiate columns).\n",
    "To cope with this issue, a custom function is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names\n",
    "column_names = ['url', 'Title', 'Label']\n",
    "\n",
    "def read_tsv(product_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Custom function to read tsv data files, in order to overcome multiple delimiters inside data.\n",
    "    \n",
    "    Args:\n",
    "        product_name (str): the name of the product dataset to load.\n",
    "    \n",
    "    Returns:\n",
    "        A Pandas DataFrame.\n",
    "    \"\"\"\n",
    "    # Dataset filepath\n",
    "    filepath = DATA_FOLDER + PRODUCTS[product_name]\n",
    "    \n",
    "    # Create and empty dataset\n",
    "    df = pd.DataFrame(columns=column_names)\n",
    "    \n",
    "    # Temporary variables to store row data\n",
    "    row_data_urls = list()\n",
    "    row_data_titles = list()\n",
    "    row_data_labels = list()\n",
    "    \n",
    "    # For each line of the file...\n",
    "    with open(filepath) as fp:\n",
    "        for i, line in enumerate(fp):\n",
    "            # Split line on all whitspaces\n",
    "            splitted_line = line.split()\n",
    "            \n",
    "            # A line should be splittable in at least 3 parts:\n",
    "            # a row has to hold at least 3 column\n",
    "            if len(line) < 3:\n",
    "                print(f'Error at line {i}: line not splittable in three parts.')\n",
    "                continue\n",
    "            \n",
    "            # The first field is the url\n",
    "            row_data_urls.append(splitted_line[0])\n",
    "            \n",
    "            # The ones in the middle are the 'Title'\n",
    "            row_data_titles.append(' '.join(splitted_line[1:-2]))\n",
    "            \n",
    "            # The last one is the 'Label'\n",
    "            row_data_labels.append(int(splitted_line[-1]))\n",
    "    \n",
    "    # Populate the dataset with the row data\n",
    "    df = pd.DataFrame({\n",
    "        'url': row_data_urls,\n",
    "        'Title': row_data_titles,\n",
    "        'Label': row_data_labels,\n",
    "    }, columns=column_names)\n",
    "    \n",
    "    return df.copy()\n",
    "    \n",
    "\n",
    "# Load ONEPLUS data\n",
    "df_oneplus = read_tsv('ONEPLUS')\n",
    "\n",
    "# Load MAVIC PRO data\n",
    "df_mavic = read_tsv('MAVIC PRO')\n",
    "\n",
    "# Load MAVIC PRO data\n",
    "df_roomba = read_tsv('ROOMBA 900 BRUSH')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring data\n",
    "We've already a lot of information about the data, but just to be sure let's confirm the datasets sizes and labels numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Num samples</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>MAVIC PRO</td>\n",
       "      <td>5335</td>\n",
       "      <td>1422</td>\n",
       "      <td>3913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ONEPLUS</td>\n",
       "      <td>5612</td>\n",
       "      <td>1959</td>\n",
       "      <td>3653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ROOMBA 900 BRUSH</td>\n",
       "      <td>696</td>\n",
       "      <td>329</td>\n",
       "      <td>367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Num samples  Positive  Negative\n",
       "MAVIC PRO                5335      1422      3913\n",
       "ONEPLUS                  5612      1959      3653\n",
       "ROOMBA 900 BRUSH          696       329       367"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution_data_positive = list()\n",
    "distribution_data_negative = list()\n",
    "\n",
    "# Get the number of positive and negative samples for each dataset\n",
    "for df in [df_mavic, df_oneplus, df_roomba]:\n",
    "    distribution_data_positive.append(df.loc[df['Label'] == 0, 'Label'].count())\n",
    "    distribution_data_negative.append(df.loc[df['Label'] == 1, 'Label'].count())\n",
    "\n",
    "# Create a simple table for better visualization (and understanding)\n",
    "pd.DataFrame({\n",
    "    'Num samples': [len(df_mavic), len(df_oneplus), len(df_roomba)], \n",
    "    'Positive': distribution_data_positive,\n",
    "    'Negative': distribution_data_negative,\n",
    "}, index=list(list(PRODUCTS.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the text contained in the _Title_ column, it'd be useful to see how many (unique) words there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of unique words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>MAVIC PRO</td>\n",
       "      <td>91689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ONEPLUS</td>\n",
       "      <td>109672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ROOMBA 900 BRUSH</td>\n",
       "      <td>13458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Number of unique words\n",
       "MAVIC PRO                          91689\n",
       "ONEPLUS                           109672\n",
       "ROOMBA 900 BRUSH                   13458"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a simple table for better visualization (and understanding)\n",
    "pd.DataFrame({\n",
    "    'Number of unique words': [df['Title'].apply(lambda x: len(x.split(' '))).sum() for df in [df_mavic, df_oneplus, df_roomba]],\n",
    "}, index=list(list(PRODUCTS.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing values\n",
    "Before the next step, we should consider the possibilty of missing values in our datasets.\n",
    "We've checked and there are none. Nevertheless, let's put a checkpoint just in case.\n",
    "let's take a moment to check if our datasets have missing values that could create problems with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not sum([df.isnull().sum().sum() for df in [df_mavic, df_oneplus, df_roomba]]), \\\n",
    "    'There shouldn\\'t be missing values. Check the datasets.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "### Text pre-processing\n",
    "For these particular datasets, our text cleaning steps includes: remove html quotation mark, remove stop words, change text to lower case, remove punctuation, remove symbols as parenthesis, hastag, etc.\n",
    "The removal of unimportant entities would increase the accuracy, because size of sample space of possible features set decreases.\n",
    "\n",
    "When dealing with document or sentences, it's also useful to normalize words with techniques like Lemmatization or Stemmming. However, here we're dealing with title of products that are usually not made by a lot of words, usually simple ones, few verb conjugations, we'll not apply these tecniques for now.  \n",
    "Also, bad products could differ from the real one by small derivations (Mavics instead of Mavic) and we'd like the model to capture these differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use english stopwords for now\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def text_cleaning(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Perform the following cleaning steps to a specific text:\n",
    "     - remove symbols,\n",
    "     - remove '&quot;' symbol (html quotation mark),\n",
    "     - remove stop words,\n",
    "     - change text to lower case,\n",
    "     - remove punctuation.\n",
    "     \n",
    "    Args:\n",
    "        text (str): text to be cleaned.\n",
    "    \n",
    "    Returns:\n",
    "        A cleaned text.\n",
    "    \"\"\"\n",
    "    BAD_SYMBOLS_RE = re.compile('[!\"#$%&\\'()*+,-./:;<=>?@\\[\\]^_`{|}~]')\n",
    "    \n",
    "    # Delete bad symbols\n",
    "    text = BAD_SYMBOLS_RE.sub('', text)\n",
    "    \n",
    "    # Remove '&quot;' symbol\n",
    "    text = text.replace('&quot;', '')\n",
    "    \n",
    "    # Stopwords removal\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
    "    \n",
    "    # To lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "df_oneplus['Title'] = df_oneplus['Title'].apply(text_cleaning)\n",
    "df_mavic['Title'] = df_mavic['Title'].apply(text_cleaning)\n",
    "df_roomba['Title'] = df_roomba['Title'].apply(text_cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: another thing that could be done to improve our models is try not to remove the words (or numbers, symbols, etc) contained in the product name.  \n",
    "For example, if the product name is 'The Roomba', 'The' is an important part of the name and thus should not be deleted (in this case it is, since 'The' is contained in the english stopwords)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: another approach could be leave *Label* numeric (0s and 1s) and convert the values into the catogories at prediction time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction: convert words into numerical features\n",
    "In order to run a machine learning model we need to convert titles and hence words into numerical features vectors.\n",
    "Here, a model called _Bag of Words_ is used, where a vector represents the frequency of a word in a predefined dictionary of words.\n",
    "We will convert text to a matrix of token counts (using *CountVectorizer*), then transform a count matrix to a normalized tf-idf representation (using *TfidfTransformer*).  \n",
    "We'll use Scilkit-Learn *Pipelines* to make the process cleaner and reproducible for all the models and products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "# Create the pre-processing pipeline\n",
    "# note: we define here a 'placeholder' clf that will be replaced later\n",
    "pipeline = Pipeline(steps = [('vect', CountVectorizer()),\n",
    "                             ('tfidf', TfidfTransformer()),\n",
    "                             ('clf', None),\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: A next step could be specify *max_features* parameter for *CountVectorizer*, limiting the number of terms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance metrics\n",
    "The more common metrics are the following:\n",
    "* *Accuracy* is the most intuitive performance metric: it’s the number of correct predictions over all predictions made. While it’s easy to interpret the accuracy, it’s doesn’t give enough information about the instances the classifier missed.\n",
    "* *Precision* is the fraction of relevant instances among the retrieved instances. Usually this metric is used when we care about reducing the false positives.\n",
    "* *Recall* (also called *Sensitivity* or *True positive rate*) is the fraction of relevant instances that have been retrieved over the total amount of relevant instances. Recall is usually used when we care about reducing the false negatives.\n",
    "* *Precision-Recall Trade-off*: one can easily have a 100% recall by classifying every sample as positive. But in doing so, you’ll have a lot of False positives,thus having a very low precision. It's pobbile then to get a better sense of this trade-off by looking at the precision recall curve.\n",
    "* *f1-score* it’s the harmonic mean of Precision and Recall. It gives more weights to lower values and favors values that are equal. A higher f1_score will appear only if the precision and recall are high too.\n",
    "\n",
    "Note that all the metrics except accuracy are computed towards one label and we want to be sure that the positive (0) label are our true positive.\n",
    "To evaluate the model performance we'll use Stratified KFold cross-validation using 5 folds, which means roughly 20% of the data is used for testing. We use the stratified version to preserve the percentage of samples for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "\n",
    "# Define custom scoring function for supporting\n",
    "# class-based precision and recall\n",
    "scoring_metrics = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'precision': make_scorer(precision_score, average='binary', pos_label=0),\n",
    "    'recall': make_scorer(recall_score, average='binary', pos_label=0),\n",
    "    'f1_score': make_scorer(f1_score, average='binary', pos_label=0),\n",
    "}\n",
    "\n",
    "# Define the validation function using (Stratified)KFold\n",
    "def model_validation(pipeline: Pipeline, df: pd.DataFrame, n_folds: int = 5) -> dict:\n",
    "    X, y = df['Title'], df['Label']\n",
    "    \n",
    "    kf = StratifiedKFold(n_folds, shuffle=True, random_state=42).get_n_splits(X)\n",
    "        \n",
    "    score = cross_validate(pipeline, X, y, cv=kf, scoring=scoring_metrics)\n",
    "        \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-test splitting\n",
    "Before starting with the next section, let's split our datasets for training and testing.\n",
    "The first set will again be splitted and use for validation using the (Stratified)K-Fold CV.\n",
    "Instead, the test set will be used for the predictions and the computational cost evaluation at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For MAVIC PRO\n",
    "temp_copy = df_mavic.copy()\n",
    "df_mavic_train_val = temp_copy.sample(frac=0.80, random_state=42)\n",
    "df_mavic_test = temp_copy.drop(df_mavic_train_val.index)\n",
    "\n",
    "# For ONEPLUS\n",
    "temp_copy = df_oneplus.copy()\n",
    "df_oneplus_train_val = temp_copy.sample(frac=0.80, random_state=42)\n",
    "df_oneplus_test = temp_copy.drop(df_oneplus_train_val.index)\n",
    "\n",
    "# For ROOMBA\n",
    "temp_copy = df_roomba.copy()\n",
    "df_roomba_train_val = temp_copy.sample(frac=0.80, random_state=42)\n",
    "df_roomba_test = temp_copy.drop(df_roomba_train_val.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection\n",
    "The models that we're going to test are:\n",
    "* *Multinomial Naive Bayes*,\n",
    "* *Linear Support Vector Machine*,\n",
    "* *Random Forest*.\n",
    "\n",
    "Each model has to be execute on all the three different products dataset.\n",
    "For this purpose, we've created a function that:\n",
    "1. takes the model pipeline has input,\n",
    "2. replace the current model being tested inside the pipeline,\n",
    "3. for each product, runs the model using the validation method described above,\n",
    "4. create a report for the defined metrics using a table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes\n",
    "The Naive Bayes classifier is a simple probabilistic classifier which is based on Bayes theorem with strong and naïve independence assumptions. It is one of the most basic text classification techniques and despite the naïve design and oversimplified assumptions it usually performs well in many complex real-world problems.    \n",
    "Though it is often outperformed by more advanced techniques, it's less computationally intensive (in both CPU and memory) and it requires a small amount of training data.  \n",
    "The Multinomial variation takes into account the number of occurrences of a term in training documents from class c, including multiple occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Accuracy  Precision  Recall  F1-score\n",
      "MAVIC PRO            0.975      0.941   0.971     0.956\n",
      "ONEPLUS              0.986      0.961   0.999     0.980\n",
      "ROOMBA 900 BRUSH     0.885      0.816   0.985     0.892\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB \n",
    "\n",
    "def model_selection(model: Pipeline) -> pd.DataFrame:\n",
    "    # Remove last classifer from the pipeline\n",
    "    pipeline.steps.pop(2)\n",
    "    \n",
    "    # And replace it with the current classifier\n",
    "    pipeline.steps.append(['clf', model])\n",
    "    \n",
    "    # temporary list for all the products\n",
    "    temporary_store = list()\n",
    "    \n",
    "    # For each product/dataset\n",
    "    for df in [df_mavic_train_val, df_oneplus_train_val, df_roomba_train_val]:\n",
    "        # Run (Stratified)KFold\n",
    "        scores = model_validation(pipeline, df)\n",
    "        \n",
    "        # Store product-data\n",
    "        product_result = pd.Series({\n",
    "            'Accuracy': scores['test_accuracy'].mean(),\n",
    "            'Precision': scores['test_precision'].mean(),\n",
    "            'Recall': scores['test_recall'].mean(),\n",
    "            'F1-score': scores['test_f1_score'].mean(),\n",
    "        })\n",
    "        \n",
    "        temporary_store.append(product_result)\n",
    "                    \n",
    "    # Create the final table\n",
    "    return pd.DataFrame(temporary_store, index=list(PRODUCTS.keys()))\n",
    "\n",
    "\n",
    "# Run model selection for Multinomial Naive Bayes\n",
    "model_res = model_selection(MultinomialNB())\n",
    "print(model_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results we can say that:\n",
    "* the overall accuracy is pretty good (see [Uncommonly-high-accuracy](Coding_Test_-_Listing_title_classifier.ipynb#Uncommonly-high-accuracy) paragraph).\n",
    "* The accuracy is lower for the Roomba dataset, which is understandable since the size is almost ten times less.\n",
    "* More or less with all three cases we notice a lower precision than recall. Remember that recall measure what should have been predicted as positive but we flagged as negative.  \n",
    "\n",
    "Therefore, this model enhance recall over precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Support Vector Machine\n",
    "Support Vector Machines determines the best decision boundary (best hyperplane) between vectors that belong to a given group and vectors that do not belong to it (divides the space into two subspaces).  \n",
    "SVM is very effective in text-mining tasks, particularly due to its effectiveness in dealing with high-dimensional data (SVM does not use all the data points or vectors to choose the boundary).  \n",
    "However, they usually are memory-intensive during training time due to the fact that a NxN kernel matrix has to be stored. If not, you'll need to recompute the kernel values repeatedly, making the training much slower.\n",
    "At prediction time, SVM takes a linear combination of all support vectors so, if there are a lot of them it'll required a lot of memory.  \n",
    "\n",
    "We'll uses Scikit-Learn _SGDClassifier_ (with 'hinge' loss) because it implements stochastic gradient descent instead of the exact gradient descent and may generalize better (downside is it may not converge to the same solution as an exact method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Accuracy  Precision  Recall  F1-score\n",
      "MAVIC PRO            0.995      0.993   0.989     0.991\n",
      "ONEPLUS              0.999      0.999   0.998     0.998\n",
      "ROOMBA 900 BRUSH     0.977      0.985   0.966     0.975\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Run model selection for Linear Support Vector Machin\n",
    "model_res = model_selection(SGDClassifier())\n",
    "print(model_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the SVM the accuracy is higher than with the previous model, even if it is by a little amount.\n",
    "Besides, this differ from the previous model since the precision is usually higher than the recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusions: precision - recall trade-off\n",
    "What would we rather have: predict as ASSET a NON-ASSET product (False Positive) or the other way around (False Negative)?\n",
    "Since it's better to flag an ASSET product a NON-ASSET product and then manually removed (if it's too much it can cause problem of course) than not flag an NON-ASSET that in reality it is an ASSET (and we've miss it!), we choose the second model. Moreover, it'd be not good if the recall was very low, but since the difference is very minimal, this should work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uncommonly high accuracy\n",
    "At first look, we notice that the metrics are overall very high, several times nearly 100%. This is very uncommon and seldom occurs in standard classifcation tasks.\n",
    "What could be wrong? Well, this scanario may occur when:\n",
    "* the classification problem is rather easy,\n",
    "* test and training data are too much alike compared to practical scenarios (so the model will probably generalize bad),\n",
    "* re-classifying training data in the test steps (some train data is leaked into the train data),\n",
    "* the classifier is overfitting the training set.\n",
    "\n",
    "A train/validation/test set splitting or a K-fold cross validation should avoid the latter case. We apply a (Stratified) 5-fold c.v. with data shuffling. This should also prevent train data leaking into train data, since the splitting is completly handled by sklearn methods.  \n",
    "A helpful approach could be to try to visualize data (both training and testing) to get an indication on how complicated the problem is, if the testing data is too similar to the training data, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model selection: a graphical representation\n",
    "Usually for model performance (and comparision) a ROC curve is used. Instead, when dealing with unbalanced classes a Precion-Recall curve is preferred.\n",
    "In our case, a ROC curve is not feasible, since SGDClassifier with hinge loss cannot predict the probabilities required to construct a ROC curve. \n",
    "For the Precion-Recall curve, instead, since the nature of the plot and the very close values a rapresentation of this type is not very useful.\n",
    "We'll use instead a box-plot of the F1-score (that takes into consideration both precision and recall) values for all the fold (see points in the plot), for the ROOMBA dataset, the one that holds more difference with the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_scores(scores:list, title: str):\n",
    "    # Construct DataFrame for visualization\n",
    "    scores_df = pd.DataFrame(scores, columns=['Model', 'Fold id', 'F1-score'],)\n",
    "    \n",
    "    # Set the size\n",
    "    fig, _ = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    # Plot data\n",
    "    sns.boxplot(x='Model', y='F1-score', data=scores_df).set_title(title)\n",
    "    sns.stripplot(x='Model', y='F1-score', data=scores_df, size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\n",
    "    \n",
    "scores_data = list()\n",
    "\n",
    "# Run the comparision for the two models\n",
    "for model in [MultinomialNB(), SGDClassifier()]:\n",
    "    # Remove last classifer from the pipeline\n",
    "    pipeline.steps.pop(2)\n",
    "\n",
    "    # And replace it with the current classifier\n",
    "    pipeline.steps.append(['clf', model])\n",
    "    \n",
    "    # Run (Stratified)KFold\n",
    "    scores = model_validation(pipeline, df_roomba_train_val)\n",
    "    \n",
    "    # Store the f1-scores (one for each fold) for this model\n",
    "    for fold_idx, f1_score in enumerate(scores['test_f1_score']):\n",
    "        scores_data.append((model.__class__.__name__, fold_idx, f1_score))\n",
    "\n",
    "plot_scores(scores_data, list(PRODUCTS.keys())[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational time\n",
    "After the model selection we've decided to use the SVM as our final algorithm.\n",
    "Now, let's try to get an idea on how much time it requires to train and to predict new samples.\n",
    "We'll re-train the selected model using the entire train-val set and make predictions using samples from the (unseen) test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Training time (ms)  Prediction time (ms)\n",
      "MAVIC PRO                     84.000                 0.688\n",
      "ONEPLUS                       83.273                 0.719\n",
      "ROOMBA 900 BRUSH              13.042                 0.724\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "# Re-create the entire pipeline\n",
    "pipeline = Pipeline(steps = [('vect', CountVectorizer()),\n",
    "                             ('tfidf', TfidfTransformer()),\n",
    "                             ('clf', SGDClassifier()),\n",
    "                            ])\n",
    "\n",
    "def compute_training_prediction_time(df_train_val: pd.DataFrame, df_test: pd.DataFrame) -> dict:\n",
    "    # For semplicity, rename columns with X and y\n",
    "    X_train, y_train = df_train_val['Title'], df_train_val['Label']\n",
    "    X_test, y_test = df_test['Title'], df_test['Label']\n",
    "    \n",
    "    # Fit on the whole train(-val) set\n",
    "    t = time.time_ns()\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    fit_time = time.time_ns() - t\n",
    "        \n",
    "    # For predictions, let's randomly take 5 samples from X_test\n",
    "    # and then take the average\n",
    "    predict_times = []\n",
    "    for x in range(1, 5):\n",
    "        # Random sampling\n",
    "        random_test_sample = X_test.sample(n=1, random_state=42)\n",
    "    \n",
    "        # Predict\n",
    "        t = time.time_ns()\n",
    "        y_pred = pipeline.predict(random_test_sample)\n",
    "        pred_time = time.time_ns() - t\n",
    "        \n",
    "        predict_times.append(pred_time)\n",
    "    \n",
    "    # The timing are returned in milliseconds\n",
    "    return {\n",
    "        'Training time (ms)': fit_time / 1000000,\n",
    "        'Prediction time (ms)': np.array(predict_times).mean() / 1000000,\n",
    "    }\n",
    "\n",
    "\n",
    "timing_mavic = compute_training_prediction_time(df_mavic_train_val, df_mavic_test)\n",
    "timing_oneplus = compute_training_prediction_time(df_oneplus_train_val, df_oneplus_test)\n",
    "timing_roomba = compute_training_prediction_time(df_roomba_train_val, df_roomba_test)\n",
    "\n",
    "# Create a simple table for visualization\n",
    "timing = pd.DataFrame([timing_mavic, timing_oneplus, timing_roomba], index=list(PRODUCTS.keys()))\n",
    "print(timing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the table above, we can see that the avarage training time takes less than 100 milliseconds for the larger dataset, while less than a millisecond for predicting a sample (a title).  \n",
    "Note that the function compute the timing using time in nanoseconds since the epoch (click [here](https://docs.python.org/3/library/time.html#epoch) for more info on epoch) and thus do not consider only the (real) cpu execution time. This may cause little variations for each execution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
